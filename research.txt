Imputation: https://www.frontiersin.org/articles/10.3389/fdata.2021.693674/full#main-content

What is the difference between the feature importance I get through the normal regression analysis and permutation analysis?

The feature importance obtained from a Random Forest model and the feature importance obtained from permutation importance 
are calculated using different methodologies, each having its own strengths and weaknesses:

Random Forest Feature Importance: This measure of feature importance is based on the average reduction in the Gini impurity 
(for classification tasks) or mean squared error (for regression tasks) that results from splits on a particular feature, 
averaged over all trees in the forest. It essentially measures the overall usefulness of a feature in the model's predictions. 
The main strengths of this method are that it is fast to compute and easy to understand. However, it has several limitations,
including a bias towards features with more levels or more numerical values, and it can be overly optimistic for correlated 
features, assigning high importance to all of them.

Permutation Feature Importance: This method works by randomly shuffling the values of a single feature and measuring 
how much the model's performance decreases. This provides a direct estimate of how much the model's predictions rely on 
the feature. Permutation feature importance is generally more reliable than the built-in feature importance of 
Random Forest models, as it is less biased and gives a more accurate measure of the usefulness of each feature. However,
it can be computationally expensive for large datasets or complex models, as it requires re-fitting the model multiple times.

Therefore, while both methods can provide valuable insights into which features are important for a model's predictions,
they may sometimes give different results due to their different methodologies and biases. It can be beneficial to use 
both methods in tandem to get a comprehensive understanding of feature importance.