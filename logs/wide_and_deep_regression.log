2023-07-01 20:27:04.447635: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-07-01 20:27:04.504504: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-01 20:27:05.432767: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-01 20:27:06,205 - INFO - Starting script...
2023-07-01 20:27:09.351752: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 37906 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:00:04.0, compute capability: 8.0
Model: "model"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 deep_input (InputLayer)        [(None, 172)]        0           []                               
                                                                                                  
 dense (Dense)                  (None, 30)           5190        ['deep_input[0][0]']             
                                                                                                  
 wide_input (InputLayer)        [(None, 172)]        0           []                               
                                                                                                  
 dense_1 (Dense)                (None, 30)           930         ['dense[0][0]']                  
                                                                                                  
 wide_output (Dense)            (None, 1)            173         ['wide_input[0][0]']             
                                                                                                  
 deep_output (Dense)            (None, 1)            31          ['dense_1[0][0]']                
                                                                                                  
 concatenate (Concatenate)      (None, 2)            0           ['wide_output[0][0]',            
                                                                  'deep_output[0][0]']            
                                                                                                  
 main_output (Dense)            (None, 1)            3           ['concatenate[0][0]']            
                                                                                                  
==================================================================================================
Total params: 6,327
Trainable params: 6,327
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
2023-07-01 20:27:12.447561: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
2023-07-01 20:27:12.452330: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7feec6337c40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2023-07-01 20:27:12.452359: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA A100-SXM4-40GB, Compute Capability 8.0
2023-07-01 20:27:12.457107: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2023-07-01 20:27:12.679313: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8902
2023-07-01 20:27:12.819159: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1572/1572 - 8s - loss: 41.8529 - val_loss: 37.8963 - 8s/epoch - 5ms/step
Epoch 2/100
1572/1572 - 4s - loss: 35.5135 - val_loss: 34.8946 - 4s/epoch - 3ms/step
Epoch 3/100
1572/1572 - 4s - loss: 33.9317 - val_loss: 33.9394 - 4s/epoch - 3ms/step
Epoch 4/100
1572/1572 - 4s - loss: 32.9470 - val_loss: 36.8539 - 4s/epoch - 3ms/step
Epoch 5/100
1572/1572 - 4s - loss: 32.6888 - val_loss: 33.1987 - 4s/epoch - 3ms/step
Epoch 6/100
1572/1572 - 4s - loss: 32.3076 - val_loss: 33.2538 - 4s/epoch - 3ms/step
Epoch 7/100
1572/1572 - 4s - loss: 32.1103 - val_loss: 32.5058 - 4s/epoch - 3ms/step
Epoch 8/100
1572/1572 - 4s - loss: 31.5064 - val_loss: 32.8541 - 4s/epoch - 3ms/step
Epoch 9/100
1572/1572 - 4s - loss: 31.4899 - val_loss: 32.8626 - 4s/epoch - 3ms/step
Epoch 10/100
1572/1572 - 4s - loss: 31.1943 - val_loss: 34.9033 - 4s/epoch - 3ms/step
Epoch 11/100
1572/1572 - 4s - loss: 31.0263 - val_loss: 32.7000 - 4s/epoch - 3ms/step
Epoch 12/100
1572/1572 - 4s - loss: 30.9083 - val_loss: 34.1682 - 4s/epoch - 3ms/step
Epoch 13/100
1572/1572 - 4s - loss: 30.5584 - val_loss: 33.1329 - 4s/epoch - 3ms/step
Epoch 14/100
1572/1572 - 5s - loss: 30.4254 - val_loss: 34.3069 - 5s/epoch - 3ms/step
Epoch 15/100
1572/1572 - 4s - loss: 30.3069 - val_loss: 32.2677 - 4s/epoch - 3ms/step
Epoch 16/100
1572/1572 - 4s - loss: 30.2796 - val_loss: 33.7597 - 4s/epoch - 3ms/step
Epoch 17/100
1572/1572 - 4s - loss: 30.0526 - val_loss: 33.4521 - 4s/epoch - 3ms/step
Epoch 18/100
1572/1572 - 4s - loss: 29.7516 - val_loss: 32.3619 - 4s/epoch - 3ms/step
Epoch 19/100
1572/1572 - 4s - loss: 29.6653 - val_loss: 32.3968 - 4s/epoch - 3ms/step
Epoch 20/100
1572/1572 - 4s - loss: 29.5568 - val_loss: 32.7104 - 4s/epoch - 3ms/step
Epoch 21/100
1572/1572 - 4s - loss: 29.4915 - val_loss: 32.7824 - 4s/epoch - 3ms/step
Epoch 22/100
1572/1572 - 4s - loss: 29.2587 - val_loss: 32.5304 - 4s/epoch - 3ms/step
Epoch 23/100
1572/1572 - 4s - loss: 29.3776 - val_loss: 31.7367 - 4s/epoch - 3ms/step
Epoch 24/100
1572/1572 - 4s - loss: 28.9682 - val_loss: 33.8269 - 4s/epoch - 3ms/step
Epoch 25/100
1572/1572 - 4s - loss: 28.9997 - val_loss: 33.1731 - 4s/epoch - 3ms/step
Epoch 26/100
1572/1572 - 5s - loss: 28.7523 - val_loss: 32.3186 - 5s/epoch - 3ms/step
Epoch 27/100
1572/1572 - 5s - loss: 28.7842 - val_loss: 32.9929 - 5s/epoch - 3ms/step
Epoch 28/100
1572/1572 - 5s - loss: 28.5124 - val_loss: 32.5697 - 5s/epoch - 3ms/step
Epoch 29/100
1572/1572 - 4s - loss: 28.4445 - val_loss: 31.9151 - 4s/epoch - 3ms/step
Epoch 30/100
1572/1572 - 4s - loss: 28.2235 - val_loss: 32.7113 - 4s/epoch - 3ms/step
Epoch 31/100
1572/1572 - 4s - loss: 28.1945 - val_loss: 32.4839 - 4s/epoch - 3ms/step
Epoch 32/100
1572/1572 - 4s - loss: 28.0239 - val_loss: 32.2019 - 4s/epoch - 3ms/step
Epoch 33/100
1572/1572 - 4s - loss: 27.9563 - val_loss: 32.8940 - 4s/epoch - 3ms/step
Epoch 34/100
1572/1572 - 4s - loss: 27.8773 - val_loss: 32.3211 - 4s/epoch - 3ms/step
Epoch 35/100
1572/1572 - 5s - loss: 27.6184 - val_loss: 33.2772 - 5s/epoch - 3ms/step
Epoch 36/100
1572/1572 - 4s - loss: 27.6964 - val_loss: 32.9756 - 4s/epoch - 3ms/step
Epoch 37/100
1572/1572 - 4s - loss: 27.4730 - val_loss: 34.7616 - 4s/epoch - 3ms/step
Epoch 38/100
1572/1572 - 4s - loss: 27.4624 - val_loss: 35.8577 - 4s/epoch - 3ms/step
2023-07-01 20:30:03,890 - INFO - Wide & Deep model saved to models/wide_and_deep_regression/wide_and_deep_model.h5.
  1/393 [..............................] - ETA: 38s 40/393 [==>...........................] - ETA: 0s  83/393 [=====>........................] - ETA: 0s123/393 [========>.....................] - ETA: 0s164/393 [===========>..................] - ETA: 0s204/393 [==============>...............] - ETA: 0s245/393 [=================>............] - ETA: 0s287/393 [====================>.........] - ETA: 0s329/393 [========================>.....] - ETA: 0s371/393 [===========================>..] - ETA: 0s393/393 [==============================] - 1s 1ms/step
2023-07-01 20:30:05,466 - INFO - Plot saved as results/wide_and_deep_regression/wide_and_deep_regression.png.
2023-07-01 20:30:05,466 - INFO - Script finished.

real	3m3.940s
user	4m16.595s
sys	0m30.136s
