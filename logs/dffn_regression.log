2023-07-01 20:34:43.171350: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-07-01 20:34:43.230682: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-01 20:34:44.156689: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-01 20:34:44,933 - INFO - Starting script...
2023-07-01 20:34:48.122957: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 37906 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:00:04.0, compute capability: 8.0
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense (Dense)               (None, 64)                11072     
                                                                 
 dense_1 (Dense)             (None, 64)                4160      
                                                                 
 dense_2 (Dense)             (None, 64)                4160      
                                                                 
 dense_3 (Dense)             (None, 64)                4160      
                                                                 
 dense_4 (Dense)             (None, 1)                 65        
                                                                 
=================================================================
Total params: 23,617
Trainable params: 23,617
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
2023-07-01 20:34:51.145268: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
2023-07-01 20:34:51.147838: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7f43bec0c450 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2023-07-01 20:34:51.147864: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA A100-SXM4-40GB, Compute Capability 8.0
2023-07-01 20:34:51.152737: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2023-07-01 20:34:51.362196: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8902
2023-07-01 20:34:51.509167: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1258/1258 - 7s - loss: 41.5869 - val_loss: 39.7380 - 7s/epoch - 6ms/step
Epoch 2/100
1258/1258 - 3s - loss: 36.0721 - val_loss: 32.1826 - 3s/epoch - 3ms/step
Epoch 3/100
1258/1258 - 3s - loss: 34.5865 - val_loss: 33.5259 - 3s/epoch - 3ms/step
Epoch 4/100
1258/1258 - 3s - loss: 33.6322 - val_loss: 34.3100 - 3s/epoch - 3ms/step
Epoch 5/100
1258/1258 - 3s - loss: 33.2160 - val_loss: 32.9578 - 3s/epoch - 3ms/step
Epoch 6/100
1258/1258 - 3s - loss: 32.4798 - val_loss: 32.4737 - 3s/epoch - 3ms/step
Epoch 7/100
1258/1258 - 3s - loss: 32.2325 - val_loss: 32.2620 - 3s/epoch - 3ms/step
Epoch 8/100
1258/1258 - 3s - loss: 31.8525 - val_loss: 30.5905 - 3s/epoch - 3ms/step
Epoch 9/100
1258/1258 - 3s - loss: 31.4517 - val_loss: 31.3420 - 3s/epoch - 3ms/step
Epoch 10/100
1258/1258 - 3s - loss: 31.5367 - val_loss: 32.0209 - 3s/epoch - 3ms/step
Epoch 11/100
1258/1258 - 3s - loss: 30.6756 - val_loss: 32.3850 - 3s/epoch - 3ms/step
Epoch 12/100
1258/1258 - 3s - loss: 30.6160 - val_loss: 32.1237 - 3s/epoch - 3ms/step
Epoch 13/100
1258/1258 - 3s - loss: 29.9910 - val_loss: 30.0880 - 3s/epoch - 3ms/step
Epoch 14/100
1258/1258 - 3s - loss: 29.4286 - val_loss: 32.6965 - 3s/epoch - 3ms/step
Epoch 15/100
1258/1258 - 3s - loss: 29.2665 - val_loss: 32.7235 - 3s/epoch - 3ms/step
Epoch 16/100
1258/1258 - 3s - loss: 28.9424 - val_loss: 30.8673 - 3s/epoch - 3ms/step
Epoch 17/100
1258/1258 - 3s - loss: 28.7363 - val_loss: 31.5920 - 3s/epoch - 3ms/step
Epoch 18/100
1258/1258 - 3s - loss: 28.4038 - val_loss: 31.5629 - 3s/epoch - 3ms/step
Epoch 19/100
1258/1258 - 3s - loss: 27.8349 - val_loss: 33.7183 - 3s/epoch - 3ms/step
Epoch 20/100
1258/1258 - 3s - loss: 27.6438 - val_loss: 30.8622 - 3s/epoch - 3ms/step
Epoch 21/100
1258/1258 - 3s - loss: 27.3245 - val_loss: 31.0050 - 3s/epoch - 3ms/step
Epoch 22/100
1258/1258 - 3s - loss: 27.0154 - val_loss: 29.8785 - 3s/epoch - 3ms/step
Epoch 23/100
1258/1258 - 3s - loss: 26.8467 - val_loss: 30.3532 - 3s/epoch - 3ms/step
Epoch 24/100
1258/1258 - 3s - loss: 26.1449 - val_loss: 31.8198 - 3s/epoch - 3ms/step
Epoch 25/100
1258/1258 - 3s - loss: 26.2128 - val_loss: 31.1312 - 3s/epoch - 3ms/step
Epoch 26/100
1258/1258 - 3s - loss: 25.7688 - val_loss: 31.0443 - 3s/epoch - 3ms/step
Epoch 27/100
1258/1258 - 3s - loss: 25.4892 - val_loss: 31.7186 - 3s/epoch - 3ms/step
Epoch 28/100
1258/1258 - 3s - loss: 25.0866 - val_loss: 31.0509 - 3s/epoch - 3ms/step
Epoch 29/100
1258/1258 - 3s - loss: 24.7637 - val_loss: 32.2640 - 3s/epoch - 3ms/step
Epoch 30/100
1258/1258 - 3s - loss: 24.6121 - val_loss: 31.9462 - 3s/epoch - 3ms/step
Epoch 31/100
1258/1258 - 3s - loss: 24.8917 - val_loss: 31.1828 - 3s/epoch - 3ms/step
Epoch 32/100
1258/1258 - 3s - loss: 23.9204 - val_loss: 31.5535 - 3s/epoch - 3ms/step
Epoch 33/100
1258/1258 - 3s - loss: 23.5843 - val_loss: 31.7710 - 3s/epoch - 3ms/step
Epoch 34/100
1258/1258 - 3s - loss: 23.4377 - val_loss: 32.7393 - 3s/epoch - 3ms/step
Epoch 35/100
1258/1258 - 3s - loss: 22.8988 - val_loss: 32.7859 - 3s/epoch - 3ms/step
Epoch 36/100
1258/1258 - 3s - loss: 23.0620 - val_loss: 31.6159 - 3s/epoch - 3ms/step
Epoch 37/100
1258/1258 - 3s - loss: 22.7493 - val_loss: 31.7210 - 3s/epoch - 3ms/step
2023-07-01 20:36:59,980 - INFO - DFFN model saved to models/dffn_regression/dffn_model.h5.
  1/393 [..............................] - ETA: 34s 46/393 [==>...........................] - ETA: 0s  90/393 [=====>........................] - ETA: 0s134/393 [=========>....................] - ETA: 0s177/393 [============>.................] - ETA: 0s219/393 [===============>..............] - ETA: 0s261/393 [==================>...........] - ETA: 0s303/393 [======================>.......] - ETA: 0s345/393 [=========================>....] - ETA: 0s388/393 [============================>.] - ETA: 0s393/393 [==============================] - 1s 1ms/step
2023-07-01 20:37:01,512 - INFO - Plot saved as results/dffn_regression/dffn_regression_plot.png.
2023-07-01 20:37:01,512 - INFO - Script finished.

real	2m21.233s
user	3m6.231s
sys	0m19.708s
