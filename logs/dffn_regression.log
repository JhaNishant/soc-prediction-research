2023-06-27 14:42:03.895683: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-06-27 14:42:03.952987: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-06-27 14:42:04.885414: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-06-27 14:42:05,665 - INFO - Starting script...
2023-06-27 14:42:08.825103: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 37908 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:00:04.0, compute capability: 8.0
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense (Dense)               (None, 64)                11072     
                                                                 
 dense_1 (Dense)             (None, 64)                4160      
                                                                 
 dense_2 (Dense)             (None, 64)                4160      
                                                                 
 dense_3 (Dense)             (None, 64)                4160      
                                                                 
 dense_4 (Dense)             (None, 1)                 65        
                                                                 
=================================================================
Total params: 23,617
Trainable params: 23,617
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
2023-06-27 14:42:11.883672: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
2023-06-27 14:42:11.886702: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7f3d600206c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2023-06-27 14:42:11.886729: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA A100-SXM4-40GB, Compute Capability 8.0
2023-06-27 14:42:11.895610: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2023-06-27 14:42:12.113429: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8902
2023-06-27 14:42:12.261023: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1258/1258 - 7s - loss: 40.9360 - val_loss: 39.0663 - 7s/epoch - 6ms/step
Epoch 2/100
1258/1258 - 3s - loss: 35.8692 - val_loss: 32.2819 - 3s/epoch - 3ms/step
Epoch 3/100
1258/1258 - 3s - loss: 34.6964 - val_loss: 33.7796 - 3s/epoch - 3ms/step
Epoch 4/100
1258/1258 - 3s - loss: 33.6875 - val_loss: 34.5325 - 3s/epoch - 3ms/step
Epoch 5/100
1258/1258 - 3s - loss: 33.3484 - val_loss: 32.8237 - 3s/epoch - 3ms/step
Epoch 6/100
1258/1258 - 3s - loss: 32.6417 - val_loss: 33.4407 - 3s/epoch - 3ms/step
Epoch 7/100
1258/1258 - 3s - loss: 32.5849 - val_loss: 32.4851 - 3s/epoch - 3ms/step
Epoch 8/100
1258/1258 - 3s - loss: 32.0280 - val_loss: 30.7279 - 3s/epoch - 3ms/step
Epoch 9/100
1258/1258 - 3s - loss: 31.4234 - val_loss: 31.6611 - 3s/epoch - 3ms/step
Epoch 10/100
1258/1258 - 3s - loss: 31.6861 - val_loss: 31.3526 - 3s/epoch - 3ms/step
Epoch 11/100
1258/1258 - 3s - loss: 30.7686 - val_loss: 32.4473 - 3s/epoch - 3ms/step
Epoch 12/100
1258/1258 - 3s - loss: 30.4941 - val_loss: 33.3288 - 3s/epoch - 3ms/step
Epoch 13/100
1258/1258 - 3s - loss: 30.1522 - val_loss: 31.1386 - 3s/epoch - 3ms/step
Epoch 14/100
1258/1258 - 3s - loss: 29.7892 - val_loss: 32.2169 - 3s/epoch - 3ms/step
Epoch 15/100
1258/1258 - 3s - loss: 29.2402 - val_loss: 33.1777 - 3s/epoch - 3ms/step
Epoch 16/100
1258/1258 - 3s - loss: 29.0541 - val_loss: 31.5762 - 3s/epoch - 3ms/step
Epoch 17/100
1258/1258 - 3s - loss: 28.7102 - val_loss: 31.9621 - 3s/epoch - 3ms/step
Epoch 18/100
1258/1258 - 3s - loss: 28.3441 - val_loss: 34.6780 - 3s/epoch - 3ms/step
Epoch 19/100
1258/1258 - 3s - loss: 27.7771 - val_loss: 32.9684 - 3s/epoch - 3ms/step
Epoch 20/100
1258/1258 - 3s - loss: 27.5652 - val_loss: 31.6487 - 3s/epoch - 3ms/step
Epoch 21/100
1258/1258 - 3s - loss: 26.9706 - val_loss: 30.6880 - 3s/epoch - 3ms/step
Epoch 22/100
1258/1258 - 3s - loss: 26.6639 - val_loss: 30.3762 - 3s/epoch - 3ms/step
Epoch 23/100
1258/1258 - 3s - loss: 26.7139 - val_loss: 31.5641 - 3s/epoch - 3ms/step
Epoch 24/100
1258/1258 - 3s - loss: 26.2104 - val_loss: 31.8610 - 3s/epoch - 3ms/step
Epoch 25/100
1258/1258 - 3s - loss: 25.8445 - val_loss: 32.9273 - 3s/epoch - 3ms/step
Epoch 26/100
1258/1258 - 3s - loss: 25.4680 - val_loss: 31.0581 - 3s/epoch - 3ms/step
Epoch 27/100
1258/1258 - 3s - loss: 24.9994 - val_loss: 31.4300 - 3s/epoch - 3ms/step
Epoch 28/100
1258/1258 - 3s - loss: 24.6945 - val_loss: 30.9267 - 3s/epoch - 3ms/step
Epoch 29/100
1258/1258 - 3s - loss: 24.4882 - val_loss: 33.2358 - 3s/epoch - 3ms/step
Epoch 30/100
1258/1258 - 3s - loss: 24.3312 - val_loss: 33.8559 - 3s/epoch - 3ms/step
Epoch 31/100
1258/1258 - 3s - loss: 24.2153 - val_loss: 31.3792 - 3s/epoch - 3ms/step
Epoch 32/100
1258/1258 - 3s - loss: 23.8772 - val_loss: 31.5810 - 3s/epoch - 3ms/step
Epoch 33/100
1258/1258 - 3s - loss: 23.2710 - val_loss: 32.4655 - 3s/epoch - 3ms/step
Epoch 34/100
1258/1258 - 3s - loss: 23.1626 - val_loss: 32.2642 - 3s/epoch - 3ms/step
Epoch 35/100
1258/1258 - 3s - loss: 22.3617 - val_loss: 35.2677 - 3s/epoch - 3ms/step
Epoch 36/100
1258/1258 - 3s - loss: 22.5588 - val_loss: 31.1956 - 3s/epoch - 3ms/step
Epoch 37/100
1258/1258 - 3s - loss: 22.1807 - val_loss: 31.9514 - 3s/epoch - 3ms/step
2023-06-27 14:44:19,870 - INFO - DFFN model saved to models/dffn_regression/dffn_model.h5.
  1/393 [..............................] - ETA: 40s 42/393 [==>...........................] - ETA: 0s  84/393 [=====>........................] - ETA: 0s127/393 [========>.....................] - ETA: 0s169/393 [===========>..................] - ETA: 0s211/393 [===============>..............] - ETA: 0s254/393 [==================>...........] - ETA: 0s296/393 [=====================>........] - ETA: 0s339/393 [========================>.....] - ETA: 0s382/393 [============================>.] - ETA: 0s393/393 [==============================] - 1s 1ms/step
2023-06-27 14:44:21,420 - INFO - Plot saved as results/dffn_regression/dffn_regression_plot.png.
2023-06-27 14:44:21,420 - INFO - Script finished.

real	2m20.401s
user	3m4.988s
sys	0m20.281s
